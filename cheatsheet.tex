\documentclass[letterpaper, 8pt]{extarticle}

\usepackage{steven}
\usepackage[margin=1.5cm]{geometry}
\usepackage{physics}
\usepackage{oubraces}
\usepackage{parskip}
\usepackage{fancyhdr}
\usepackage{multicol}
\usepackage{titlesec}
\usepackage{mathtools}
\usepackage[nodisplayskipstretch]{setspace}

\setstretch{0.1}

\pagestyle{fancy}
\fancyhf{}
\setlength{\headheight}{20pt}
\setlength{\headsep}{8pt}
\lhead{STA261 Cheatsheet}
\rhead{Steven Tran}

\titlespacing{\section}{0pt}{\parskip}{\parskip}

\titleformat{\section}
	{\normalfont\upshape\bfseries}{\thesection}{1em}{}

\pagenumbering{gobble}
\setlength\parindent{0pt}
\hfuzz=\maxdimen

\begin{document}
\begin{multicols*}{3}
	\section{STA257 Basics}
	Set stuff
	\begin{gather*}
		A,B\text{ disjoint}\Leftrightarrow A\cap B=\emp \\
		A\cup B=B\cup A \\
		(A\cup B)\cup C=A\cup (B\cup C) \\
		(A\cup B)\cap C=(A\cap C)\cup (B\cap C)
	\end{gather*}
	Probability Measure:
	\begin{gather}
		P(\Omega)=1 \\
		A\subset\Omega\IMP P(A)\geq 0\\
		A_i\text{ mutually disjoint}\scriptstyle\IMP P\mqty(\bigcup_{i=1}^nA_i)=\sum_{i=1}^nP(A_i)
	\end{gather}
	\text{PMF} $p(x_i)$\quad\text{PDF} $f(x_i)$\quad CDF $F(x_i)$
	\begin{gather*}
		\shortintertext{Conditional Probability}
		P(A|B)=P(A\cap B)/P(B)
		\shortintertext{Law of Total Probability ($\bigcup_{i=1}^nB_i=\Omega,B_i$ disjoint, $P(B_i)>0$)}
		P(A)=\sum_{i=1}^nP(A|B_i)P(B_i)
		\shortintertext{Bayes': $A,B_i$ disjoint, $\sum_{i=1}^nB_i=\Omega,P(B_i)>0$, then}
		P(B_j|A)=\frac{P(A|B_j)P(B_j)}{\sum_{i=1}^nP(A|B_i)P(B_i)}
		\shortintertext{Multinomial Coefficient: group $n$ objects into $k$ classes, each of size $n_i$}
		\binom{n}{n_1,\cdots,n_k}=\frac{n!}{n_1!n_2!\cdots n_k!} 
		\shortintertext{Binomial Theorem}
		(a+b)^n=\sum_{k=0}^n\binom{n}{k}a^kb^{n-k}
	\end{gather*}
	\section{Distributions}
	Bernouilli: success ($p$) or failure ($1-p$) with $p\IN[0,1]$. $X\sim Ber(p)$ then
	\[p(x)=\begin{cases}p^x(1-p)^{1-x}, &x=0, 1\\0&\text{otherwise}\end{cases}\]
	Expectation $p$ Variance $(1-p)p$ MGF $q+pe^t$
	
	Binomial: $n$ $Ber(p)$ trials with $k$ successes. $X\sim Bin(n, p)$ then
	\[p(k)=\binom{n}{k}p^k(1-p)^{n-k}\]
	E $np$\quad V $npq$\quad MGF $(1-p+pe^t)^n$
	
	Geometric: $k$ $Ber(p)$ trials until 1 success. $X\sim Geo(p)$ then 
	\[p(k)=p(1-p)^{k-1}\]
	E $\frac{1}{p}$\quad V $\frac{1-p}{p^2}$\quad MGF $\frac{pe^t}{1-(1-p)e^t}$
	\begin{gather*}
		\sum_{n=1}^\infty az^{n-1}=\sum_{n=0}^\infty az^n \\
		\sum_{n=1}^ka_nr^{n-1}=a_n\mqty(\frac{1-r^k}{1-r})
	\end{gather*}
	Neg. Binomial: $Ber(p)$ trials conducted until $r$ successes. $X\sim NB(r,p)$ then
	\[p(k)=\binom{k-1}{r-1}p^r(1-p)^{k-r}\]
	E $\frac{r}{p}$\quad V $\frac{rp}{(1-p)^2}$\quad MGF $\mqty(\frac{1-p}{1-pe^t})^2$
	
	Hypergeometric: $X\sim HG(n,m,r)$ where $n$ is the total number of items, $m$ is the number of items sampled, and $r$ is the number of items with a property, then
	\[p(k)=\frac{\binom{r}{k}\binom{n-r}{m-k}}{\binom{n}{m}} \tag{$k\IN 0,\cdots, \min(r,m)$}\]
	E $\frac{mr}{n}$
	
	Poisson: \# events. $X\sim Poi(\text{rate}=\lambda)$ then
	\[p(k)=\frac{\lambda^ke^{-\lambda}}{k!}\]
	E $\lambda$\quad V $\lambda$\quad MGF $e^{\lambda(e^t-1)}$
	
	Properties of Poisson:
	\begin{itemize}
		\item For $|S_i|=N_i$ independent $Poi(\lambda)$, $N_i\sim Poi(\lambda|S_i|)$
		\item For $n$ large, $Bin(n,p)\sim Poi(np)$ can be approximated
	\end{itemize}
	
	Exponential ($Gamma(1,\lambda)$): $X$ waiting time $\sim Exp(\lambda)$, then 
	\[f(x)=\lambda e^{-\lambda x}\quad F(x)=1-e^{-\lambda x}\]
	E $\frac{1}{\lambda}$\quad V $\frac{1}{\lambda^2}$\quad M $\frac{\lambda}{\lambda-t}$
	
	Gamma (sum $a$ iid $Exp(\lambda)$): $X\sim \Gamma(a,\lambda)$ then
	\[f(x)=\frac{\lambda^a}{\Gamma(a)}x^{a-1}e^{-x\lambda}\]
	E $\frac{a}{\lambda}$\quad V $\frac{a}{\lambda^2}$\quad M $\mqty(\frac{\lambda}{\lambda-t})^a$
	
	Beta: used to model proportions between $0$ and $1$ with $a,b>0$ shape parameters. $X\sim Beta(a,b)$ then
	\[f(x)=\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}x^{a-1}(1-x)^{b-1}\]
	E $\frac{a}{a+b}$\quad V $\frac{ab}{(a+b)^2(a+b+1)}$
	
	Normal: $X\sim N(\mu, \sigma^2)$ then 
	\begin{gather*}
		f(x)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}} \\
		\text{MGF } e^{\mu t+\frac{\sigma^2t^2}{2}}\quad f(\mu-x)=f(\mu+x) \\
		\frac{X-\mu}{\sigma}\sim N(0,1)
	\end{gather*}
	Std. Normal $Z\sim N(0,1)$ $\phi(z)$ given in table
	
	$X_i\sim N(\mu_i,\sigma^2_i$ for $i\IN\{1,\cdots,n\}$. $Y=\sum_{i=1}^na_iX_i+b$, then
	\[Y\sim N\mqty(\displaystyle \sum_{i=1}^na_i\mu_i+b,\sum_{i=1}^na_i^2\sigma_i^2)\]
	
	Chi-Square distribution: $U=Z^2$ where $Z\sim N(0,1)$, then $U\sim\chi^2_{df=1}$. If $X\sim \chi^2_{(m)}$ and $Y\sim \chi^2_{(n)}$ then $X+Y\sim \chi^2_{(m+n)}$. If $X\sim\chi^2_{(m)}$ then $E(X)=m$.
	
	$t$ distribution: $Z\sim N(0,1)\perp U\sim\chi^2_{(m)}$ then $\frac{Z}{\sqrt{\frac{U}{m}}}\sim t_{(m)}$, the $t$ distribution with $m$ degrees of freedom
	
	$F$ distribution: $X\sim\chi^2_{(m)}\perp Y\sim\chi^2_{(n)}$ then $\frac{\frac{X}{m}}{\frac{Y}{n}}\sim F(m,n)$.
	
	\section{Inequalities, Expectation, Variance, MGFs}
	
	Markov's Inequality: $P(X\geq 0)=1, E(X)$ exists then 
	\[P(X\geq t)\leq \frac{E(X)}{t}\]
	Chebyshev's Inequality: $P(|X-\mu|>t)\leq\frac{\sigma^2}{t^2}$ (proof: set $Y=(x-\mu)^2$ and apply Markov))
	\begin{gather*}
		\text{r-th moment}=E(X^r) \\
		\text{r-th central moment}=E\mqty[(X-E(X))^r]\\
		M(t)=E(e^{tX})\\
		X\perp Y\IMP M_{X+Y}=M_XM_Y\\
		M_{XY}(s,t)=E(e^{sX+tY})\\
		M^{(r)}(0)=E(X^r)\\
		E(X)=\sum_ix_ip(x_i)\quad E(X)=\int_\R xf(x)\dd x \tag{provided these converge} \\
		Y=g(X)\IMP E(Y)=\sum_ig(x_i)p(x_i) \\ E(Y)=\int_\R g(x)f(x)\dd x \\
		E(aX+b)=aE(X)+E(b)\quad E(XY)=E(X)E(Y) \tag{if $X\perp Y$} \\
		Var(X)=E\mqty[(X-E(X))^2]\\
		=E(X^2)-(E(X))^2=\int_\R(x-\mu)f(x)\dd x \\
		sd=\sqrt{Var(X)} \\
		Y=aX+b\IMP Var(Y)=a^2Var(X)
	\end{gather*}
	\section{Conditional \& Multivariate Stuff}
	Law of Total Expectation
	\begin{gather*}
		E(Y)=E(E(Y|X)) 
		\shortintertext{Law of Total Variance}
		Var(Y)=Var(E(Y|X))+E(Var(Y|X))\\
		p_{xy}(x,y)=p_{X|Y}(x|y)p_y(y)\\
		p_x(x)=\int_yp_{xy}(x,y)\dd y\\
		E(Y)=\idotsint \underbrace{g(x_1,\cdots,x_n)}_\text{may do nothing}f(x_1,\cdots,x_n)\dd \{x_i\}\\
		E(Y|X=x)=\sum_yp_{Y|X}(y|x) \\
		E(h(Y)|X=x)=\int_yh(Y)f_{Y|X}(y|x)\dd y\\
		\shortintertext{$X_i\perp X_j$ then $Var(\sum x_i)=\sum Var(X_i)$ and $Cov(X_i,X_j)=0$)}\\
		Cov(X,Y)=E\mqty[(X-\mu_X)(Y-\mu_Y)]\\
		=E(XY)-E(X)E(Y)\\
		Cov(a+X,Y)=Cov(X,Y)\\
		Cov(aX,bY)=abCov(X,Y)\\
		Cov(X,Y+Z)=Cov(X,Y)+Cov(X,Z)\\
		Var(X+Y)=Var(X)+Var(Y)+2Cov(X,Y)\\
		\begin{align*}
			Cov(aW+bX,cY+dZ)&=acCov(W,Y)\\
			&+bcCov(X,Y)\\
			&+adCov(W,Z)\\
			&+bdCov(X,Z)
		\end{align*}\\
		Var(a+\sum b_ix_i)=\sum_{i,j}b_ib_jCov(x_i,x_j)
	\end{gather*}
	\newpage
	Posterior density:
	\[f_{P|X}(p|x)=\frac{f_{X,P}(x,p)}{f_X(x)}=\frac{f_{X|P}(x|p)f_P(p)}{f_X(x)}\]
	\section{Limit Theorems}
	Law of Large Numbers: $X_1,X_2,\cdots,X_i,\cdots$ independent and $E(X_i)=\mu$, $Var(X_i)=\sigma^2$, $\bar X_n=\frac{1}{n}\sum_{i=1}^nX_i$. 
	
	Then $\forall\epsilon>0, P(|\bar X_n-\mu|>\epsilon)\overset{n\to\infty}{\longrightarrow} 0$ by Chebyshev's inequality.
	
	Convergence in Distribution: $X_1,\cdots$ are random variables with $F_1,\cdots$, and $X$ has cdf $F$. $X_n\overset{D}{\longrightarrow}X$ if $F_n(X)\overset{D}{\rightarrow}F(X)$ wherever $F$ is continuous.
	\begin{itemize}
		\item the next outcome (as we get more and more $X_i$s) converge closer and closer to some cdf
		\item to show converge in distribution, we usually use MGFs. Call $\{F_n\}$ a sequence of cdfs with MGFs $\{M_n\}$. 
		\[\overbrace{M_N(t)\to M(t)}^{\forall t\IN I\sth 0\IN t}\IMP F_n(x)\to F(x)\]
		since the MGF uniquely determines the distribution of a RV.
	\end{itemize}
	
	Central Limit Theorem: $X_1,\cdots$ iid with mean $\mu$, variance $\sigma^2$, cdf $F$, MGF $M$ defined in a neighbourhood of $0$. Let $S_n=\sum_{i=1}^n X_i$. Then $\bar X_n\overset{D}{\to}N(\mu,\frac{\sigma^2}{n})$ or $\frac{\bar X_n-\mu}{\frac{\sigma}{\sqrt n}}\overset{D}{\to}N(0,1)$ or $P\mqty(\frac{S_n}{\sigma\sqrt x}\leq x)\rightarrow\phi(x)$.
	
	Proof: $M_{S_n}(t)=\mqty(M_x(t))^n,M_{Z_n}(t)=\mqty(M_x\mqty(\frac{t}{\sigma\sqrt x}))^n,M_X(s)=M_X(0)+SM'(0)+\frac{S^2}{2}M''(0)+\epsilon_S$ with $\frac{\epsilon_S}{S^2}\to 0$. This equals $1+\frac{1}{2}\sigma^2+\mqty(\frac{t}{\sigma\sqrt x}))^2+\epsilon_n$, so $M_{Z_n}(t)=\mqty(1+\frac{t^2}{2n}+\epsilon_n)^n\to e^{\frac{t^2}{2}}$ hence $Z_n\sim N(0,1)$.
	
	\section{Definitions}
	\begin{itemize}
		\item Population: a collection of all the subjects that have something in common.
		\item[$\to$] Parameter: a characteristic/summary of the population, represented by $\theta$. Can be mean ($\mu$), std. dev ($\sigma$), etc.
		\item Sample: a subset of the population. We use the sample to make an inference about the unknown parameters of our population.
		\item[$\to$] Statistic: any summary of the sample; since statistics/estimators are a function of sample observations, we use $T$ to represent them. Examples: sample total ($\sum X_i$), sample mean ($\bar X$), etc.
	\end{itemize}
	\begin{center}	
	\begin{tabular}{|c|c|c|}
		\hline
		parameter & estimator & estimate \\\hline
		$\mu$ & $\bar X=\frac{\sum X}{n}$ & $\bar x=\frac{\sum x}{n}$ \\
		$\sigma$ & $S$ & $s$\\\hline
	\end{tabular}
	\end{center}
	
	\section{Method of Moments}
	$X_1,\cdots,X_n$ iid RVs. Define the k-th population moment to be $\mu_k=E(X^k)$ and the k-th sample moment to be $\hat\mu_k=\frac{1}{n}\sum_{i=1}^nX_i^k$. We use $\hat\mu_k$ as an estimator of $\mu_k$ using 3 steps:
	\begin{enumerate}
		\item express lower order population moments in terms of the parameters
		\item invert the expressions to express the parameters in terms of the population moments
		\item replace the population moments using the sample moments
	\end{enumerate}
	\section{Likelihood}
	$X_1,\cdots,X_n$ RVs with joint density/mass function $f(x_1,\cdots,x_n|\theta)$. Given a sample $(x_1,\cdots,x_n)$, the likelihood function of $\theta$ is defined as
	\[L(\theta):=L(\theta|x_1,\cdots,x_n)=f(x_1,\cdots,x_n|\theta)\]
	where the likelihood is intuitively the probabilility of the parameter being some value given the sample data. If $X_1,\cdots,X_n$ are iid, then we can express the joint as the product of the marginal densities, i.e.
	\[L(\theta)=\prod_{i=1}^nf_\theta(x_i)\]
	Suppose we have $\theta$ with likelihood function $L(\theta)$. The best point estimate can be found by picking a $\hat\theta$ that maximizes $L(\theta)$, i.e. $\hat\theta$ satisfies $L(\hat\theta)\geq L(\theta)\quad \forall\theta\IN\Omega$.
	
	Usually, we compute the MLE by optimizing the log-likelihood $\ell(\theta)$ ($\ln x$ is one-to-one and increasing). Solve $\pdv{\ell(\theta)}{\theta}=0$ for $\theta$ and check that $\left.\pdv[2]{\ell(\theta)}{\theta}\right|_{\theta=\hat\theta}<0$.
	
	Invariance property: suppose $\hat\theta$ is the MLE of $\theta$ and let $\psi(\theta)$ be any 1-1 function of $\theta$ defined on $\Omega$, then $\psi(\hat\theta)$ is the MLE of $\psi(\theta)$.
	
	Fisher Information
	\begin{align*}
		I(\theta)&=E\mqty[\displaystyle\pdv{\theta}\ln f(X|\theta)]^2
		\shortintertext{also, if $f$ is sufficiently smooth (in order to bring operation in integration when finding expectation),}
		&=-E\mqty[\displaystyle\pdv[2]{\theta}\ln f(X|\theta)]
	\end{align*}
	
	The large sample distribution of a MLE is approximately normal with mean $\theta_0$ and variance $\frac{1}{nI(\theta_0)}$. Moreover, the asymptotic variance is given by
    \[\underbrace{Var(\hat\theta)\geq\frac{1}{nI(\theta_0)}}_\text{Cram\'er-Rao Bound}=-\frac{1}{E\ell''(\theta_0)}\]
    
    Proof: consider the correlation coefficient $\rho$ between two variables $Y,Z$, which is bounded between $-1$ and $1$.
    Then
    \begin{align*}
        \rho^2(T,S()\theta))&\leq 1 \\
        \frac{(\text{cov}(T,S(\theta)))^2}{\text{var}(T)\text{var}(S(\theta))}&\leq 1 \\
        \IMP \text{var(T)}&\geq\frac{(\text{cov}(T,S(\theta)))^2}{\text{var}(S(\theta))}
    \end{align*}
	
	\section{Mean Squared Error \& Bias}
	Let $\theta$ be a parameter, $\psi(\theta)$ be a real-valued function, and $T$ be an estimator of $\psi(\theta)$. The Mean Squared Error is defined as
	\begin{align*}
		MSE_\theta(T)&=E_\theta[(T-\psi(\theta))^2\\
		&=Var_\theta(T)+(E_\theta(T)-\psi(\theta))^2 \tag{*}\\
		&=Var_\theta(T)+(Bias(T))^2
	\end{align*}
	Proof (*): Add $-E(T)+E(T)$ to inner term in definition of MSE, expand using squares.
	\[\text{Bias}:=E_\theta(T)-\psi(\theta)\]
	When the bias of an estimator is 0, it is unbiased.
	
	\section{Quiz 1 Problems}
	\begin{enumerate}[label=\alph*), wide, labelwidth=0pt, labelindent=0pt]
		\item (Rice E8Q4) Suppose $X$ is a discrete random variable with $P(X=0,1,2,3)=\frac{2}{3}\theta,\frac{1}{3}\theta,\frac{2}{3}(1-\theta),\frac{1}{3}(1-\theta)$ respectively where $\theta\IN[0,1]$ and 10 observations were taken: $(3,0,2,1,3,2,1,0,2,1)$. 
		
		Method of moments estimate of $\theta$: $E(X)=\sum_{k=0}^3kP(X=k)=\frac{\theta}{3}+\frac{4}{3}(1-\theta)+(1-\theta)=\frac{7}{3}-2\theta$. We rearrange for $\theta$ and write the sample mean $\bar X$ in place of $E(X)$: $\hat\theta=\frac{7}{6}-\frac{1}{2}\bar X$ which yields $\hat\theta=0.417$.
		
		Standard error: we need to calculate the variance of $X$ ($Var(X)=E(X^2)-(E(X))^2$). The process yields $E(X^2)=\frac{17-16\theta}{3}$ so $Var(X)=-4\theta^2+4\theta+\frac{2}{9}$. $Var(\bar X)=Var(\frac{1}{n}\sum_{i=1}^nX_i)=\frac{1}{n^2}Var(X_i)=\frac{1}{n}Var(X_1)$. 
		
		$Var(\hat\theta)=\frac{1}{4}Var(\bar X)=-\frac{1}{10}\theta^2+\frac{1}{10}\theta+\frac{1}{180}$. We replace $\theta$ with $\hat\theta=0.417$, yielding that $s_{\hat\theta}^2=0.0299$ and the standard deviation is simply the square of this.
		
		\item (Rice E8Q19) Suppose $X_1,\cdots,X_n$ are iid $N(\mu, \sigma^2)$. MLE of each of $\sigma,\mu$, with the other one known: 
		
		$L(\theta)=\frac{1}{\sigma^n(2\pi)^\frac{n}{2}}e^{-\frac{1}{2}}\left(\sum_{i=1}^n\left(\frac{x_i-\mu}{\sigma}\right)^2\right)$. Log likelihood: $\ell(\theta)=-n\ln(\sigma)-\frac{n}{2}\ln(2\pi)-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2$ and then maximize this function for each parameter. We get that $\hat\sigma=\sqrt{\frac{1}{n}\sum_{i=1}^n(x_i-\mu)^2}$ and $\hat\mu=\frac{\sum_{i=1}^nx_i}{n}$.
		
		\item (Rice E8Q7) Suppose $X\sim Geo(p)$ and we have an iid sample of size $n$. Method of moments estimate of $p$: we are looking to express $p$ in terms of the moments; we have that $E(X)=p$ so $p=\frac{1}{E(X)}$. Hence, $\hat p=\frac{1}{\bar X}$.
		
		MLE of $p$: $L(p)=p^n(1-p)^{\sum_{i=1}^n(x_i-1)}$, $\ell(p)=n\ln(p)+\sum_{i=1}^n(x_i-1)\ln(1-p)$ which we can use to maximize $p$: $p=\frac{n}{\sum_{i=1}^nx_i}=\frac{1}{\bar X}$, obtaining that $\bar p=\frac{1}{\bar X}$.
		
		Variance of our MLE: $Var(\hat p)=\frac{-1}{E(\ell''(p)}$ asymptotically and we have that $E(\ell''(p))=E\left(-\frac{n}{p^2}-\left[\sum_{i=1}^nX_i-n\right]\frac{1}{(1-p)^2}\right)=-\frac{n}{p^2}-\left(\sum_{i=1}^nE(X_i)-n\right)\frac{1}{(1-p)^2}=-\frac{n}{p^2(1-p)}$. Hence $Var(\hat p)\approx\frac{p^2(1-p)}{n}$.
		
		If $p$ has a uniform prior distribution on $[0,1]$, the posterior distribution of $p$ is $f_{P|X}(p|x)=\frac{f_{X|P}(x|p)f_p(p)}{f_X(x)}$. $f_{X|P}(x|p)$ is simply the likelihood function, and $f_X(x)$ can be computed: $f_X(x)=\int_\R f_{X|P}(x|p)f_P(p)\dd p=\int_0^1f_{X|P}(x|p)\dd p=\int_0^1p^n(1-p)^{\sum_{i=1}^n(x_i-1)}\dd p$. Relating this to a Beta distribution, we find a value for $f_X(x)$ so we plug it into $F_{P|X}(p|x)$.
		
		The expected value of our posterior distribution is given by $E(P|X)=\frac{n}{n+\sum_{i=1}^nx_i-n}$ (again, using the mean of a Beta distribution).
	\end{enumerate}
	
	\newpage
	\section{Types of Convergence}
	\begin{itemize}
		\item Convergence in distribution: $Z_n\overset{d}{\to}Z$ if $\displaystyle\lim_{n\to\infty}F_n(x)=F(x)$ everywhere $F$ is continuous
		\item Convergence in probability: $Z_n\overset{P}{\to}Z$ if $\displaystyle\lim_{n\to\infty}P(|Z_n-Z)|>\epsilon)=0$
		\item Almost-surely convergence: $Z_n\overset{a.s.}{\to}Z$ if $P(\displaystyle\lim_{n\to\infty}Z_n=Z)=1$
	\end{itemize}
	\section{Unbiased Estimator for $\sigma^2$}
	We have that the population variance is
	\[\sigma^2=\frac{1}{N}\sum_{i=1}^N(X_i-\mu)^2\]
	and we have may want to estimate it using $\frac{1}{n-1}\sum_{i=1}^n(X_i-\bar X)^2$ or $\frac{1}{n}\sum_{i=1}^n(X_i-\bar X)^2$. The first is unbiased, and the other one (given by both the method of moments \& MLE estimation) is biased. Proof:
	\begin{align*}
		\sum_i(X_i-\mu)^2&=\sum_i(X_i-\bar X+\bar X-\mu)^2\\
		&=\sum_i\left[(X_i-\bar X)^2+(\bar X-\mu)^2\right.\\
		&+\left.2(X_i-\bar X)(\bar X-\mu)\right]\\
		&=n\hat\sigma^2+n(\bar X-\mu)^2+0(\bar X-\mu)\\
		&=\sum_i(X_i-\bar X)^2+n(\bar X-\mu)^2\\
		\sum_i(X_i-\bar X)^2&=\sum_i(X_i-\mu)^2-n(\bar X-\mu)^2
		\shortintertext{Take expectation on both sides:}
		E[\cdots]&=\sum_iVar(X_i)-nVar(\bar X)\\
		&=\sum_i\sigma^2-n\frac{\sigma^2}{n}=n\sigma^2-\sigma^2\\
		&=(n-1)\sigma^2
	\end{align*}
	Another method for the normal distribution: it can be shown that $\bar X$ and $S^2$ are independent, and $\frac{(n-1)S^2}{\sigma^2}\sim\chi^2_{(n-1)}$. Starting with the equation before we took the expectation, we can divide it by $\sigma^2$ to obtain
	\[\sum_i\left(\frac{X_i-\mu}{\sigma}\right)^2=\frac{(n-1)S^2}{\sigma^2}+\left(\frac{\bar X-\mu}{\sigma/\sqrt n}\right)^2\]
	where $\sum_i\left(\frac{X_i-\mu}{\sigma}\right)^2\sim\chi^2_{(n)}$ and $\left(\frac{\bar X-\mu}{\sigma/\sqrt n}\right)^2\sim\chi^2_{(1)}$. So we can use MGFs and the independence of $\bar X$ and $S^2$ to find that $\frac{(n-1)S^2}{\sigma^2}\sim\chi^2_{(n-1)}$. Since the expectation of a Chi Square variable is just its degrees of freedom we have that $E(S^2)=\sigma^2$ so $S^2$ is an unbiased estimator of $\sigma^2$ under the normal distribution.
	
	\section{Sufficient Statistic}
	Roughly, a sufficient statistic for a parameter is a summary of a sample which yields the same information about the parameter as the entire sample (data reduction). Formally, a statistic $T(X_1,\cdots,X_n)$ is sufficient for $\theta$ if the conditional distribution of $X_1,\cdots,X_n$ given $T=t$ doesn't depend on $\theta$.
	
	\textbf{Factorization Theorem}: $T(X_1,\cdots,X_n)$ is sufficient for $\theta$ if the joint probability function factors in the form
	\begin{align*}
		&f(x_1,\cdots,x_n|\theta) \\
		=\ &g(T(x_1,\cdots,x_n),\theta)h(x_1,\cdots,x_n)
	\end{align*}
	where $h$ is a function of sample observations and $g$ involves $\theta$ and the sufficient statistic $T$.
	
	It is also useful to identify distributions in the exponential family of distributions. We can express the density or frequency functions in the form
	
	\[f(x|\theta)=\begin{cases}
	e^{c(\theta)T(x)+d(\theta)+S(x)}, & x\IN A\\
	0, & x\not\in A
	\end{cases}\]
	Such distributions include the  normal, exponential, log-normal, gamma, chi-squared, beta, Dirichlet, Bernoulli, categorical, Poisson, geometric, inverse Gaussian, von Mises and von Mises-Fisher distributions. Some distributions are exponential families only if some of their parameters are held fixed. {\tiny The family of Pareto distributions with a fixed minimum bound $x_m$ form an exponential family. The families of binomial and multinomial distributions with fixed number of trials n but unknown probability parameter(s) are exponential families. The family of negative binomial distributions with fixed number of failures (a.k.a. stopping-time parameter) r is an exponential family. However, when any of the above-mentioned fixed parameters are allowed to vary, the resulting family is not an exponential family.} As mentioned above, as a general rule, the support of an exponential family must remain the same across all parameter settings in the family.
	
	\section{Consistent Estimators}
	In this course, consider only estimators being consistent in probability. 
	
	Immediate results: the LLN tells use that $\bar X=\frac{1}{n}\sum X_i\overset{P}{\to}E(X_i)$ for any distribution, so $\bar X$ is a consistent estimator of $\mu$ for $X_i\overset{iid}{\sim}N(\mu,\sigma^2)$, $\bar X$ is a consistent estimator of $\lambda$ for $X_i\overset{iid}{\sim}Poi(\lambda)$, etc.
	
	\textbf{Slutsky's lemma}: $X_n, Y_n$ are sequences.
	\begin{gather*}
		X_n\overset{P}{\to}X, Y_n\overset{P}{\to}Y\IMP X_n+Y_n\overset{P}{\to}X+Y \\
		X_n\overset{P}{\to}X, Y_n\overset{P}{\to}Y\IMP X_nY_n\overset{P}{\to}XY
	\end{gather*}
	\textbf{Continuous mapping theorem}: $X_n\overset{P}{\to}X$ and $g$ continuous. Then $g(X_n)\overset{P}{\to}g(X)$
	
	\textbf{MSE consistent}: an estimator $T_n$ is MSE consistent if $MSE(T_n)\overset{n\to\infty}{\longrightarrow}0$
	
	We have that the MLE is consistent. Let $\theta_0$ represent the true value of the parameter which produced the data (unknown, constant) let $X_i\overset{iid}{\sim} f(x|\theta_0)$, and let $\hat\theta$ be the MLE. Proof that $\hat\theta\overset{P}{\to}\theta_0$: start with the log likelihood $\ell(\theta)=\sum_{i=1}^n\log f(X_i|\theta)$. Divide both sides by the sample size $n$ and apply LLN.
	
	Since $\frac{1}{n}\ell(\theta)$ gets closer to $E(\log f(X_i|\theta))$, the $\theta$ that maximizes $\frac{1}{n}\ell(\theta)$ should be closed to the $\theta$ that maximizes $E(\log f(X_i|\theta))$. We can show that $E(\log f(X_i|\theta))$ is maximized at $\theta_0$.
	
	\section{Score and Fisher Information}
	The score function is the derivative of the log likelihood, 
	\[S(\theta)=\pdv{\theta}\ell(\theta)=\ell'(\theta)\]
	and the solution to the score equation 
	\[S(\theta)=0\]
	is the MLE. We say that $\left.S(\theta)\right|_{\theta=\hat\theta}=0$.
	
	The score as a random variable for iid $X_i$ has that
	\begin{align*}
		S(\theta|X_i)&=\pdv{\theta}\sum_i\log f(X_i|\theta)\\
		&=\sum_i\pdv{\theta}\log f(X_i|\theta)=\sum_iS(\theta|X_i)
	\end{align*}
	They key takeaway is that $S(\theta)$ is random; for a different set of observations, the likelihood function varies. 
	
	The Fisher Information is defined by
	\begin{align*}
		I(\theta_0)&=Var\left(S(\theta|X)|_{\theta=\theta_0}\right)\\
		&=E\left(\pdv{\theta}\left.\log f(X|\theta)\right|_{\theta=\theta_0}\right)^2\\
		&=-E\left(\pdv[2]{\theta}\log f(X|\theta)|_{\theta=\theta_0}\right)
	\end{align*}
	and is the amount of information that each observable random variable $X$ contains about $\theta$.
	\section{Quiz 2 Problems}
	\begin{enumerate}[label=\alph*), wide, labelwidth=0pt, labelindent=0pt]
		\item (W4 Notes) Suppose $X_1,X_2,X_3\overset{iid}{\sim}Poi(\lambda)$. Verify that $T=\sum_{i=1}^3X_i$ is a sufficient statistic for $\lambda$.
		\begin{align*}
			&P(X_1=x_1,\cdots,X_3=x_3|\sum x_i=t)\\
			&=\frac{P(X_1=x_1,\cdots,X_3=x_3,\sum x_i=t)}{P(\sum x_i=t)}\\
			&=\frac{P(X_1=x_1,\cdots,X_3=t-(x_1+x_2))}{P(\sum x_i=t)}\\
			&=\frac{\frac{\cancel{e^{-\lambda}}\lambda^{x_1}}{x_1!}\frac{\cancel{e^{-\lambda}}\lambda^{x_2}}{x_2!}\frac{\cancel{e^{-\lambda}}\lambda^{t-(x_1+x_2)}}{(t-(x_1+x_2))!}}{\frac{\cancel{e^{-3\lambda}}(3\lambda)^t}{t!}}\\
			&=\frac{\cancel{\lambda^t}}{x_1!x_2!}\frac{t!}{(3\cancel{\lambda})^t}=\frac{t!}{x_1!x_2!3^t}
		\end{align*}
		which doesn't depend on $\lambda$.
		
		\item (W4 Notes) Factorization theorem for $Poi(\lambda)$:
		\begin{align*}
			L(\lambda)&=e^{n\lambda}\lambda^{\sum x_i}\frac{1}{\prod x_i!}\\
			&=g\left(\sum_{i=1}^nx_i,\lambda\right)h(x_1,\cdots,x_n)
		\end{align*}
		so $T=\sum x_i$ is a sufficient statistic for $\lambda$.
		
		\item (W4 Notes) Score \& Fisher Information using Poisson Distribution
		
		Let $X_1,\cdots,X_n\overset{iid}{\sim}Poi(\lambda)$ with $\lambda_0$ parameter. 
		\begin{align*}
			L(\lambda)&=\prod f(X_i|\theta)\\
			&=\prod\frac{e^{-\lambda}\lambda^{\sum X_i}}{\prod X_i!}\\
			\ell(\lambda)&=-n\lambda +(\sum X_i)\ln\lambda-\sum\ln(X_i)!\\
			\pdv{\lambda}\ell(\lambda)&=-n+\frac{\sum X_i}{\lambda}\\
			I(\lambda_0)&=Var(S(\lambda|X)|_{\lambda=\lambda_0})\\
			&=Var\left(-1+\frac{X}{\lambda_0}\right)\\
			&=\frac{1}{\lambda^2_0}Var(X)=\frac{1}{\lambda_0}
		\end{align*}
    \end{enumerate}
    \newpage
    \section{Efficiency}
    For two estimators $T_1,T_2$ of a statistic $\theta$, the efficiency of $T_1$ relative to $T_2$ is
    \[\text{eff}(T_1,T_2)=\frac{\text{var}(T_2)}{\text{var}(T_1)}\]
    A more efficient estimator has a smaller variance, and the most efficient unbiased estimator achieves the Cram\`er-Rao Lower Bound.

    \section{Distribution of MLE}
    Under some conditions, as $n\to\infty$, 
    \[\frac{\hat\theta-\theta_0}{\sqrt{\frac{1}{n}I(\theta_0)}}\overset{D}{\to}N(0,1)\]
    As a consequence, for large $n$, $E(\hat\theta)=\theta_0$ and $V(\hat\theta)=\frac{1}{nI(\theta_0)}$.

    Proof: we have that $\ell'(\hat\theta)=0, E(\ell'(\theta_0))=0), \ell'(\theta_0)\overset{D}{\to}N(0,nI(\theta_0))$ so $\frac{1}{n}\ell'(I(\theta_0))\overset{D}{\to}N(0,\frac{1}{n}I(\theta_0))$. 
    
    Additionally, by LLN, $\frac{1}{n}\ell''(\theta_0)\overset{P}{\to}-I(\theta_0)$. 

    We then use the Taylor series expansion of $\ell'(\hat\theta)$ around $\theta_0$ to find that
    \[\hat\theta-\theta_0\approx-\frac{\ell'(\theta_0)}{\ell''(\theta_0)}=\frac{\frac{1}{n}\ell'(\theta_0)}{-\frac{1}{n}\ell''(\theta_0)}\]
    And with this we obtain the result. Additionally, the MLE is: asymptotically unbiased, a function of sufficient statistics, consistent, and asymptotically efficient.

    \section{Confidence Intervals}
    Some sampling distributions:
    \begin{itemize}
        \item $X_1,\cdots,X_n\overset{iid}{\sim}N(\mu,\sigma^2)$:
        \begin{gather*}
            \frac{\bar X-\mu}{\sigma/\sqrt n}\sim N(0,1) \\
            \frac{\bar X-\mu}{S/\sqrt n}\sim t_{(n-1)} \\
            \frac{(n-1)S^2}{\sigma^2}\sim\chi^2_{(n-1)}
        \end{gather*}
        \item Distribution of MLE (above)
    \end{itemize}
    $\gamma$-Confidence interval: intuitively, some interval which will have at least $\gamma$ chance of containing the true parameter. $C(X_1,\cdots,X_n)=(l(X_1,\cdots,X_n),u(X_1,\cdots,X_n))$ is a $\gamma$ confidence interval for $\psi$ if $P(l()\leq\psi(\theta)\leq u)\geq\gamma$ for every $\theta\IN\Omega$.

    Pivotal Quantity: a random variable defined in terms of the sample; involves the unknown parameters in its expression, but the distribution of this quantity doesn't depend on the parameters.

    For each of the sampling distributions above, we can use the $z, t$, or $\chi^2$ distributions to calculate the necessary quantiles which correspond to $\gamma$. We use the standard normal for $\mu$ when $\sigma^2$ is known, the $t$ distribution for $\mu$ when $\sigma^2$ is unknown, and the $\chi^2$ distribution for $\sigma^2$. 
    
    For two-sided confidence intervals, notice that the $Z,t$ distributions are symmetric about their means so the shortest interval is also symmetric about the mean. For the $chi^2$ distribution, we use $\frac{1\pm\gamma}{2}$ even though it may not yield the shortest interval (its shape is determined by the degrees of freedom).

    For MLE-based confidence intervals, the confidence interval is given by
    \[\hat\theta\pm z_{(\frac{1+\gamma}{2})}\sqrt{\frac{1}{nI(\theta_0)}}\]
    where $\hat\theta$ is the MLE and we use either a plug-in estimate of the Fisher Information (replace $\theta_0$ by $\hat\theta$) or the observed Fisher Information (replace $\theta_0$ with summary of observed data).

    For one-sided confidence intervals, we consider one side of each distribution.

    Interpretation:
    \begin{itemize}
        \item For $z,t$ intervals, the sample mean is the midpoint of the lower and upper bound. The width of the interval is given by $u-l$, and the margin of error is half of the width.
        \item The width of the interval increases as the confidence level increases or the standard deviation ($\sigma,s$) increases
        \item The width of the interval decreases as the sample size increases
        \item CIs are not fixed numbers but rather random variables; ``there is a 95\% chance that $\mu$ is between 4.442 and 5.318 is incorrect.'' Rather, we should say, ``if we keep taking samples and construct 0.95-CIs, 95\% of the confidence intervals will capture the true value of the parameter.''
	\end{itemize}
	
	\section{Hypothesis Tests}
	Null Hypothesis, $H_0$: the hypothesis we want to test with respect to the parameter. Usually will be a simple hypothesis (parameter takes a single value from the parameter space).

	Alternative Hypothesis, $H_A$ or $H_1$: other values the parameter of interest can take on. Usually will be a composite hypothesis.

	We use test statistics which, under the null hypothesis, will have a known distribution. Some of the pivotal quantities used to construct confidence intervals are useful for the critical region or $p$-value approach.

	Critical region, $R_\alpha(T)$ ($\alpha\IN[0,1]$): a region of the distribution of the test statistic under the null hypothesis such that we reject $H_0$ if $T(X)\IN R_\alpha(T)$.
	\[P[T(X)\IN R_\alpha(T)|H_0\text{ true}]=\alpha\]

	$p$-value: the smallest level of significance at which $H_0$ would be rejected based on the observed data, or the probability of observing the result as or more extreme than what was actually observed if $H_0$ is true. We reject $H_0$ if the $p$-value is less than a cutoff (conventionally, $0.01,0.05,0.1$).

	Some example $p$-values for two-sided tests:

	\begin{gather*}
		2\left(1-\Phi\left(\left|\frac{\bar x-\mu_0}{\sigma/\sqrt n}\right|\right)\right) \tag{z test} \\
		2\left(1-G\left(\left|\frac{\bar x-\mu_0}{s/\sqrt n}\right|\right)\right) \tag{t test}
	\end{gather*}

	\begin{center}
		\begin{tabular}{c|c|c}
			\hline
			& fail to reject $H_0$ & reject $H_0$ \\\hline
			$H_0$ true & correct & type 1 ($\alpha$) \\\hline
			$H_0$ false & type 2 ($\beta$) & correct \\\hline
		\end{tabular}
	\end{center}
	Notice that $\alpha,\beta$ are inversely related. 

	Power of a test: $1-\beta$. We calculate this using the density under $H_1$; start with finding $R_a$ given $H_0$, and then calculate the probability of having data within the rejection region given $H_1$.

	\section{Tests of Two Populations}
	By constructing other test statistics, we can reduce a hypothesis test involving two population parameters to one of the previous tests.

	Consider two independent normal samples
	\begin{gather*}
		X_1,\cdots,X_n\sim N(\mu_x,\sigma^2_x) \\
		Y_1,\cdots,Y_m\sim N(\mu_y,\sigma^2_y)
	\end{gather*}
	Say we want to test $H_0:\sigma^2_x=\sigma^2_y$ vs $H_1:\sigma^2_x\neq\sigma^2_y$, then using an $F$ distribution, we can write
	\[\frac{S^2_x/\sigma^2_x}{S^2_y/\sigma^2_y}\sim F_{n-1,m-1}\]
	Under $H_0$, $\frac{S^2_x}{S^2_y}\sim F_{n-1,m-1}$ and the rejection region is $(-\infty, F_{\alpha/2,(n-1,m-1))})\cup (F_{1-\alpha/2,(n-1,m-1)}, \infty)$

	Say we want to test $H_0:\mu_x=\mu_y$ ($\mu_x-\mu_y$) vs $H_1:\mu_x\neq \mu_y$ with $\sigma_x,\sigma_y$ known. We can use $\bar X,\bar Y$ to estimate each parameter respectively, giving
	\begin{gather*}
		\bar X-\bar Y\sim N\left(\mu_x-\mu_y,\sigma^2_x/n+\sigma^2_y/m\right) \\
		\frac{(\bar X-\bar Y)-(\mu_x-\mu_y)}{\sqrt{\sigma^2_x/n+\sigma^2_y/m}}\sim N(0,1)
		\intertext{and under $H_0$}
		\frac{(\bar X-\bar Y)}{\sqrt{\sigma^2_x/n+\sigma^2_y/m}}\sim N(0,1)
	\end{gather*}
	Also if we know $\sigma_x=\sigma_y=\sigma$, we can factor out the population variance term in the denominator above.

	Say we want to test  $H_0:\mu_x=\mu_y$ ($\mu_x-\mu_y$) vs $H_1:\mu_x\neq \mu_y$ with $\sigma_x,\sigma_y$ unknown but equal. Then
	\begin{align*}
		\frac{(n-1)S^2_x}{\sigma^2}+\frac{(m-1)S^2_y}{\sigma^2}&\sim\chi^2_{(n+m-2)}\\
		\frac{1}{\sigma^2}((n-1)S^2_x+(m-1)S^2_y)&\sim\chi^2_{(n+m-2)}
		\intertext{and using the def'n of a $t$-distribution}
		\frac{\frac{\bar X-\bar Y}{\sigma\sqrt{1/n+1/m}}}{\sqrt{\frac{\frac{1}{\sigma^2}[(n-1)S^2_x+(m-1)S^2_y]}{n+m-2}}}&\sim t_{n+m-2}\\
		\frac{\bar X-\bar Y}{\sqrt{\frac{(n-1)S^2_x+(m-1)S^2_y}{n+m-2}}\sqrt{\frac{1}{n}+\frac{1}{m}}}&\sim t_{n+m-2}\\
		\frac{\bar X-\bar Y}{S_p\sqrt{\frac{1}{n}+\frac{1}{m}}}&\sim t_{n+m-2}
	\end{align*}
	we call $S^2_p=\sqrt{\frac{(n-1)S^2_x+(m-1)S^2_y}{n+m-2}}$ the pooled sample variance.

	In this case, we use the test statistic $\frac{\bar X-\bar Y}{\sqrt{\frac{S^2_x}{n}+\frac{S^2_y}{m}}}$ which is approximated using a $t$ distribution with a complicated form of the degrees of freedom (not in this course).

	For paired (dependent) data $X,Y$, we can define $D=X-Y$ and test $\mu_d=0$ using 
	\[\frac{\bar D}{S_d/\sqrt n}\sim t_{n-1}\]
\end{multicols*}
\end{document}
